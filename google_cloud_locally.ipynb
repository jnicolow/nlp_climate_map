{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f59f4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'gutils.gutils' from '/home/exouser/nlp_climate_map/gutils/gutils.py'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from google import genai\n",
    "import google.auth\n",
    "from google.genai.types import GenerateContentConfig, Part\n",
    "import httpx\n",
    "import streamlit as st\n",
    "from google.genai import types\n",
    "\n",
    "from gutils import gutils\n",
    "import importlib\n",
    "importlib.reload(gutils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed8b201b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/anaconda3/envs/geo/lib/python3.13/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "prompt = 'how was the temperature in 2019'\n",
    "\n",
    "text = gutils.answer_prompty(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad9747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "# print(text)\n",
    "assert('{' in text)\n",
    "\n",
    "\n",
    "# dict_string = text.replace(\"\\n\", '')  # replace single quotes with double quotes\n",
    "\n",
    "match = re.search(r\"\\{.*?\\}\", text, re.DOTALL)  # this gets the portion within curly brackets\n",
    "if match:\n",
    "    dict_string = match.group(0) \n",
    "    # print(dict_string)\n",
    "    # dict_string = dict_string.replace('\"', '').strip()  # Remove double quotes and strip whitespace\n",
    "\n",
    "    dict_string = dict_string.replace(\"'\", '\"')  # replace single quotes with double quotes\n",
    "    dict_string = dict_string.replace('None', '\"None\"')\n",
    "    # dict_string = re.sub(r'(\\b\\w+\\b)', r'\"\\1\"', dict_string)  # wrap words with double quotes\n",
    "    # print(dict_string)\n",
    "    json_data = json.loads(dict_string)\n",
    "    # print(json_data)\n",
    "else:\n",
    "    print(\"No dictionary found in the input string.\")\n",
    "\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ecfae27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "temperature/mean/month/statewide/data_map/2019/temperature_mean_month_statewide_data_map_2019.tif\n",
      "mean\n",
      "temperature/mean/month/statewide/data_map/2019/temperature_mean_month_statewide_data_map_2019_01.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/anaconda3/envs/geo/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ikeauth.its.hawaii.edu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "temperature/mean/month/statewide/data_map/2019/temperature_mean_month_statewide_data_map_2019_02.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/anaconda3/envs/geo/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ikeauth.its.hawaii.edu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "temperature/mean/month/statewide/data_map/2019/temperature_mean_month_statewide_data_map_2019_03.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/anaconda3/envs/geo/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ikeauth.its.hawaii.edu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "temperature/mean/month/statewide/data_map/2019/temperature_mean_month_statewide_data_map_2019_04.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/anaconda3/envs/geo/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ikeauth.its.hawaii.edu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "temperature/mean/month/statewide/data_map/2019/temperature_mean_month_statewide_data_map_2019_05.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/anaconda3/envs/geo/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ikeauth.its.hawaii.edu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "temperature/mean/month/statewide/data_map/2019/temperature_mean_month_statewide_data_map_2019_06.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/anaconda3/envs/geo/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ikeauth.its.hawaii.edu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "temperature/mean/month/statewide/data_map/2019/temperature_mean_month_statewide_data_map_2019_07.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/anaconda3/envs/geo/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ikeauth.its.hawaii.edu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(json_data[\u001b[33m'\u001b[39m\u001b[33myear\u001b[39m\u001b[33m'\u001b[39m],\u001b[38;5;28mlist\u001b[39m): json_data[\u001b[33m'\u001b[39m\u001b[33myear\u001b[39m\u001b[33m'\u001b[39m] = json_data[\u001b[33m'\u001b[39m\u001b[33myear\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m      8\u001b[39m file_download = hcdp.FileDownloadAPI(product_type=json_data[\u001b[33m'\u001b[39m\u001b[33mproduct_type\u001b[39m\u001b[33m'\u001b[39m], year=json_data[\u001b[33m'\u001b[39m\u001b[33myear\u001b[39m\u001b[33m'\u001b[39m], month=json_data[\u001b[33m'\u001b[39m\u001b[33mmonth\u001b[39m\u001b[33m'\u001b[39m], aggregation=json_data[\u001b[33m'\u001b[39m\u001b[33maggregation\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m dataset = \u001b[43mfile_download\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# file_download.plot_raster()\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# rainfall/new/month/statewide/data_map/2019/rainfall_new_month_statewide_data_map_2019_01.tif\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# temperature/new/month/statewide/data_map/2019/temperature_new_month_statewide_data_map_2019_01.tif\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp_climate_map/hcdp/hcdp.py:125\u001b[39m, in \u001b[36mFileDownloadAPI.get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.month \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.month == \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m: \n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m         \u001b[38;5;28mself\u001b[39m.dataset = \u001b[43mget_year_avg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduct_type\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproduct_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maggregation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset\n\u001b[32m    127\u001b[39m     response = requests.get(\u001b[38;5;28mself\u001b[39m.url, verify=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp_climate_map/hcdp/hcdp.py:200\u001b[39m, in \u001b[36mget_year_avg\u001b[39m\u001b[34m(product_type, year, aggregation)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m month \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m13\u001b[39m):\n\u001b[32m    199\u001b[39m     test = FileDownloadAPI(product_type, year=year, month=month, aggregation=aggregation)\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     dataset = \u001b[43mtest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m     image_data = dataset.read(\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Read the first band (assuming one band per image)\u001b[39;00m\n\u001b[32m    202\u001b[39m     images.append(image_data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp_climate_map/hcdp/hcdp.py:127\u001b[39m, in \u001b[36mFileDownloadAPI.get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m     \u001b[38;5;28mself\u001b[39m.dataset = get_year_avg(product_type=\u001b[38;5;28mself\u001b[39m.product_type, year=\u001b[38;5;28mself\u001b[39m.year, aggregation=\u001b[38;5;28mself\u001b[39m.aggregation)\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m    130\u001b[39m     file_bytes = BytesIO(response.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/geo/lib/python3.13/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/geo/lib/python3.13/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/geo/lib/python3.13/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/geo/lib/python3.13/site-packages/requests/sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/geo/lib/python3.13/site-packages/requests/models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/geo/lib/python3.13/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/geo/lib/python3.13/site-packages/urllib3/response.py:1066\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1068\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m   1069\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/geo/lib/python3.13/site-packages/urllib3/response.py:955\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    952\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) >= amt:\n\u001b[32m    953\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoded_buffer.get(amt)\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    957\u001b[39m flush_decoder = amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/geo/lib/python3.13/site-packages/urllib3/response.py:879\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    876\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    878\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[32m    882\u001b[39m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    887\u001b[39m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[32m    888\u001b[39m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[32m    889\u001b[39m         \u001b[38;5;28mself\u001b[39m._fp.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/geo/lib/python3.13/site-packages/urllib3/response.py:862\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1()\n\u001b[32m    860\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    861\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/geo/lib/python3.13/http/client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/geo/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/geo/lib/python3.13/ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/geo/lib/python3.13/ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from hcdp import hcdp\n",
    "import importlib\n",
    "importlib.reload(hcdp)\n",
    "\n",
    "if json_data['month'] == \"None\": json_data['month'] = None\n",
    "if json_data['month'] == 'null': json_data['month'] = None\n",
    "if isinstance(json_data['year'],list): json_data['year'] = json_data['year'][0]\n",
    "file_download = hcdp.FileDownloadAPI(product_type=json_data['product_type'], year=json_data['year'], month=json_data['month'], aggregation=json_data['aggregation'])\n",
    "dataset = file_download.get_data()\n",
    "# file_download.plot_raster()\n",
    "# rainfall/new/month/statewide/data_map/2019/rainfall_new_month_statewide_data_map_2019_01.tif\n",
    "# temperature/new/month/statewide/data_map/2019/temperature_new_month_statewide_data_map_2019_01.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb340621",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# ## Error Handling\n",
    "# - If the input is unclear or incomplete, the model should politely request clarification or additional details.\n",
    "# - For unknown queries, the model should acknowledge its limitations and suggest alternative ways to obtain the required information.\n",
    "# - In cases of technical issues, the model should provide a standard apology message and recommend trying again later.\n",
    "\n",
    "\n",
    "# safety_settings = [types.SafetySetting(\n",
    "    #   category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    #   threshold=\"ON\"\n",
    "    # ),types.SafetySetting(\n",
    "    #   category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    #   threshold=\"ON\"\n",
    "    # ),types.SafetySetting(\n",
    "    #   category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    #   threshold=\"ON\"\n",
    "    # ),types.SafetySetting(\n",
    "    #   category=\"HARM_CATEGORY_HARASSMENT\",\n",
    "    #   threshold=\"ON\"\n",
    "    # )],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4f99ffc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'type' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 59\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load Google Gen AI Client.\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m genai\u001b[38;5;241m.\u001b[39mClient(\n\u001b[0;32m     52\u001b[0m         vertexai\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     53\u001b[0m         project\u001b[38;5;241m=\u001b[39mPROJECT_ID,\n\u001b[0;32m     54\u001b[0m         location\u001b[38;5;241m=\u001b[39mLOCATION,\n\u001b[0;32m     55\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mAPI_KEY,\n\u001b[0;32m     56\u001b[0m     )\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model_name\u001b[39m(name: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the formatted model name.\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name:\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'type' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# pylint: disable=broad-exception-caught,broad-exception-raised,invalid-name\n",
    "\"\"\"\n",
    "This module demonstrates the usage of the Gemini API in Vertex AI within a Streamlit application.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from google import genai\n",
    "import google.auth\n",
    "from google.genai.types import GenerateContentConfig, Part\n",
    "import httpx\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "def _project_id() -> str:\n",
    "    \"\"\"Use the Google Auth helper (via the metadata service) to get the Google Cloud Project\"\"\"\n",
    "    try:\n",
    "        _, project = google.auth.default()\n",
    "    except google.auth.exceptions.DefaultCredentialsError as e:\n",
    "        raise Exception(\"Could not automatically determine credentials\") from e\n",
    "    if not project:\n",
    "        raise Exception(\"Could not determine project from credentials.\")\n",
    "    return project\n",
    "\n",
    "\n",
    "def _region() -> str:\n",
    "    \"\"\"Use the local metadata service to get the region\"\"\"\n",
    "    try:\n",
    "        resp = httpx.get(\n",
    "            \"http://metadata.google.internal/computeMetadata/v1/instance/region\",\n",
    "            headers={\"Metadata-Flavor\": \"Google\"},\n",
    "        )\n",
    "        return resp.text.split(\"/\")[-1]\n",
    "    except Exception:\n",
    "        return \"us-central1\"\n",
    "\n",
    "\n",
    "API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "PROJECT_ID = 'alohadata-team5'#os.environ.get(\"GOOGLE_CLOUD_PROJECT\", _project_id())\n",
    "LOCATION = 'us-central1'#os.environ.get(\"GOOGLE_CLOUD_REGION\", _region())\n",
    "MODELS = {\n",
    "    \"gemini-2.0-flash\": \"Gemini 2.0 Flash\",\n",
    "    \"gemini-2.0-flash-lite\": \"Gemini 2.0 Flash-Lite\",\n",
    "    \"gemini-2.5-pro-exp-03-25\": \"Gemini 2.5 Pro Experimental\",\n",
    "}\n",
    "\n",
    "\n",
    "@st.cache_resource\n",
    "def load_client() -> genai.Client:\n",
    "    \"\"\"Load Google Gen AI Client.\"\"\"\n",
    "    return genai.Client(\n",
    "        vertexai=True,\n",
    "        project=PROJECT_ID,\n",
    "        location=LOCATION,\n",
    "        api_key=API_KEY,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_model_name(name: str | None) -> str:\n",
    "    \"\"\"Get the formatted model name.\"\"\"\n",
    "    if not name:\n",
    "        return \"Gemini\"\n",
    "    return MODELS.get(name, \"Gemini\")\n",
    "\n",
    "\n",
    "st.link_button(\n",
    "    \"View on GitHub\",\n",
    "    \"https://github.com/GoogleCloudPlatform/generative-ai/tree/main/gemini/sample-apps/gemini-streamlit-cloudrun\",\n",
    ")\n",
    "\n",
    "cloud_run_service = os.environ.get(\"K_SERVICE\")\n",
    "if cloud_run_service:\n",
    "    st.link_button(\n",
    "        \"Open in Cloud Run\",\n",
    "        f\"https://console.cloud.google.com/run/detail/us-central1/{cloud_run_service}/source\",\n",
    "    )\n",
    "\n",
    "st.header(\":sparkles: Gemini API in Vertex AI\", divider=\"rainbow\")\n",
    "client = load_client()\n",
    "\n",
    "freeform_tab, tab1, tab2, tab3, tab4 = st.tabs(\n",
    "    [\n",
    "        \"Freeform\",\n",
    "        \"Generate story\",\n",
    "        \"Marketing campaign\",\n",
    "        \"Image Playground\",\n",
    "        \"Video Playground\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "with freeform_tab:\n",
    "    st.subheader(\"Enter Your Own Prompt\")\n",
    "\n",
    "    selected_model = st.radio(\n",
    "        \"Select Model:\",\n",
    "        MODELS.keys(),\n",
    "        format_func=get_model_name,\n",
    "        key=\"selected_model_freeform\",\n",
    "        horizontal=True,\n",
    "    )\n",
    "\n",
    "    temperature = st.slider(\n",
    "        \"Select the temperature (Model Randomness):\",\n",
    "        min_value=0.0,\n",
    "        max_value=2.0,\n",
    "        value=0.5,\n",
    "        step=0.05,\n",
    "        key=\"temperature\",\n",
    "    )\n",
    "\n",
    "    max_output_tokens = st.slider(\n",
    "        \"Maximum Number of Tokens to Generate:\",\n",
    "        min_value=1,\n",
    "        max_value=8192,\n",
    "        value=2048,\n",
    "        step=1,\n",
    "        key=\"max_output_tokens\",\n",
    "    )\n",
    "\n",
    "    top_p = st.slider(\n",
    "        \"Select the Top P\",\n",
    "        min_value=0.0,\n",
    "        max_value=1.0,\n",
    "        value=0.95,\n",
    "        step=0.05,\n",
    "        key=\"top_p\",\n",
    "    )\n",
    "\n",
    "    prompt = st.text_area(\n",
    "        \"Enter your prompt here...\",\n",
    "        key=\"prompt\",\n",
    "        height=200,\n",
    "    )\n",
    "\n",
    "    config = GenerateContentConfig(\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_output_tokens,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "    generate_freeform = st.button(\"Generate\", key=\"generate_freeform\")\n",
    "    if generate_freeform and prompt:\n",
    "        with st.spinner(\n",
    "            f\"Generating response using {get_model_name(selected_model)} ...\"\n",
    "        ):\n",
    "            first_tab1, first_tab2 = st.tabs([\"Response\", \"Prompt\"])\n",
    "            with first_tab1:\n",
    "                response = client.models.generate_content(\n",
    "                    model=selected_model,\n",
    "                    contents=prompt,\n",
    "                    config=config,\n",
    "                ).text\n",
    "\n",
    "                if response:\n",
    "                    st.markdown(response)\n",
    "            with first_tab2:\n",
    "                st.markdown(\n",
    "                    f\"\"\"Parameters:\\n- Model ID: `{selected_model}`\\n- Temperature: `{temperature}`\\n- Top P: `{top_p}`\\n- Max Output Tokens: `{max_output_tokens}`\\n\"\"\"\n",
    "                )\n",
    "                st.code(prompt, language=\"markdown\")\n",
    "\n",
    "with tab1:\n",
    "    st.subheader(\"Generate a story\")\n",
    "\n",
    "    selected_model = st.radio(\n",
    "        \"Select Model:\",\n",
    "        MODELS.keys(),\n",
    "        format_func=get_model_name,\n",
    "        key=\"selected_model_story\",\n",
    "        horizontal=True,\n",
    "    )\n",
    "\n",
    "    # Story premise\n",
    "    character_name = st.text_input(\n",
    "        \"Enter character name: \\n\\n\", key=\"character_name\", value=\"Mittens\"\n",
    "    )\n",
    "    character_type = st.text_input(\n",
    "        \"What type of character is it? \\n\\n\", key=\"character_type\", value=\"Cat\"\n",
    "    )\n",
    "    character_persona = st.text_input(\n",
    "        \"What personality does the character have? \\n\\n\",\n",
    "        key=\"character_persona\",\n",
    "        value=\"Mittens is a very friendly cat.\",\n",
    "    )\n",
    "    character_location = st.text_input(\n",
    "        \"Where does the character live? \\n\\n\",\n",
    "        key=\"character_location\",\n",
    "        value=\"Andromeda Galaxy\",\n",
    "    )\n",
    "    story_premise = st.multiselect(\n",
    "        \"What is the story premise? (can select multiple) \\n\\n\",\n",
    "        [\n",
    "            \"Love\",\n",
    "            \"Adventure\",\n",
    "            \"Mystery\",\n",
    "            \"Horror\",\n",
    "            \"Comedy\",\n",
    "            \"Sci-Fi\",\n",
    "            \"Fantasy\",\n",
    "            \"Thriller\",\n",
    "        ],\n",
    "        key=\"story_premise\",\n",
    "        default=[\"Love\", \"Adventure\"],\n",
    "    )\n",
    "    creative_control = st.radio(\n",
    "        \"Select the creativity level: \\n\\n\",\n",
    "        [\"Low\", \"High\"],\n",
    "        key=\"creative_control\",\n",
    "        horizontal=True,\n",
    "    )\n",
    "    length_of_story = st.radio(\n",
    "        \"Select the length of the story: \\n\\n\",\n",
    "        [\"Short\", \"Long\"],\n",
    "        key=\"length_of_story\",\n",
    "        horizontal=True,\n",
    "    )\n",
    "\n",
    "    if creative_control == \"Low\":\n",
    "        temperature = 0.30\n",
    "    else:\n",
    "        temperature = 0.95\n",
    "\n",
    "    if length_of_story == \"Short\":\n",
    "        max_output_tokens = 2048\n",
    "    else:\n",
    "        max_output_tokens = 8192\n",
    "\n",
    "    prompt = f\"\"\"Write a {length_of_story} story based on the following premise: \\n\n",
    "  character_name: {character_name} \\n\n",
    "  character_type: {character_type} \\n\n",
    "  character_persona: {character_persona} \\n\n",
    "  character_location: {character_location} \\n\n",
    "  story_premise: {\",\".join(story_premise)} \\n\n",
    "  If the story is \"short\", then make sure to have 5 chapters or else if it is \"long\" then 10 chapters.\n",
    "  Important point is that each chapters should be generated based on the premise given above.\n",
    "  First start by giving the book introduction, chapter introductions and then each chapter. It should also have a proper ending.\n",
    "  The book should have prologue and epilogue.\n",
    "  \"\"\"\n",
    "    config = GenerateContentConfig(\n",
    "        temperature=temperature, max_output_tokens=max_output_tokens\n",
    "    )\n",
    "\n",
    "    generate_t2t = st.button(\"Generate my story\", key=\"generate_t2t\")\n",
    "    if generate_t2t and prompt:\n",
    "        with st.spinner(\n",
    "            f\"Generating your story using {get_model_name(selected_model)} ...\"\n",
    "        ):\n",
    "            first_tab1, first_tab2 = st.tabs([\"Story\", \"Prompt\"])\n",
    "            with first_tab1:\n",
    "                response = client.models.generate_content(\n",
    "                    model=selected_model,\n",
    "                    contents=prompt,\n",
    "                    config=config,\n",
    "                ).text\n",
    "\n",
    "                if response:\n",
    "                    st.write(\"Your story:\")\n",
    "                    st.write(response)\n",
    "            with first_tab2:\n",
    "                st.markdown(\n",
    "                    f\"\"\"Parameters:\\n- Model ID: `{selected_model}`\\n- Temperature: `{temperature}`\\n- Max Output Tokens: `{max_output_tokens}`\\n\"\"\"\n",
    "                )\n",
    "                st.code(prompt, language=\"markdown\")\n",
    "\n",
    "with tab2:\n",
    "    st.subheader(\"Generate your marketing campaign\")\n",
    "\n",
    "    selected_model = st.radio(\n",
    "        \"Select Model:\",\n",
    "        MODELS,\n",
    "        format_func=get_model_name,\n",
    "        key=\"selected_model_marketing\",\n",
    "        horizontal=True,\n",
    "    )\n",
    "\n",
    "    product_name = st.text_input(\n",
    "        \"What is the name of the product? \\n\\n\", key=\"product_name\", value=\"ZomZoo\"\n",
    "    )\n",
    "    product_category = st.radio(\n",
    "        \"Select your product category: \\n\\n\",\n",
    "        [\"Clothing\", \"Electronics\", \"Food\", \"Health & Beauty\", \"Home & Garden\"],\n",
    "        key=\"product_category\",\n",
    "        horizontal=True,\n",
    "    )\n",
    "    st.write(\"Select your target audience: \")\n",
    "    target_audience_age = st.radio(\n",
    "        \"Target age: \\n\\n\",\n",
    "        [\"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65+\"],\n",
    "        key=\"target_audience_age\",\n",
    "        horizontal=True,\n",
    "    )\n",
    "    target_audience_location = st.radio(\n",
    "        \"Target location: \\n\\n\",\n",
    "        [\"Urban\", \"Suburban\", \"Rural\"],\n",
    "        key=\"target_audience_location\",\n",
    "        horizontal=True,\n",
    "    )\n",
    "    st.write(\"Select your marketing campaign goal: \")\n",
    "    campaign_goal = st.multiselect(\n",
    "        \"Select your marketing campaign goal: \\n\\n\",\n",
    "        [\n",
    "            \"Increase brand awareness\",\n",
    "            \"Generate leads\",\n",
    "            \"Drive sales\",\n",
    "            \"Improve brand sentiment\",\n",
    "        ],\n",
    "        key=\"campaign_goal\",\n",
    "        default=[\"Increase brand awareness\", \"Generate leads\"],\n",
    "    )\n",
    "    if campaign_goal is None:\n",
    "        campaign_goal = [\"Increase brand awareness\", \"Generate leads\"]\n",
    "    brand_voice = st.radio(\n",
    "        \"Select your brand voice: \\n\\n\",\n",
    "        [\"Formal\", \"Informal\", \"Serious\", \"Humorous\"],\n",
    "        key=\"brand_voice\",\n",
    "        horizontal=True,\n",
    "    )\n",
    "    estimated_budget = st.radio(\n",
    "        \"Select your estimated budget ($): \\n\\n\",\n",
    "        [\"1,000-5,000\", \"5,000-10,000\", \"10,000-20,000\", \"20,000+\"],\n",
    "        key=\"estimated_budget\",\n",
    "        horizontal=True,\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"Generate a marketing campaign for {product_name}, a {product_category} designed for the age group: {target_audience_age}.\n",
    "  The target location is this: {target_audience_location}.\n",
    "  Aim to primarily achieve {campaign_goal}.\n",
    "  Emphasize the product's unique selling proposition while using a {brand_voice} tone of voice.\n",
    "  Allocate the total budget of {estimated_budget}.\n",
    "  With these inputs, make sure to follow following guidelines and generate the marketing campaign with proper headlines: \\n\n",
    "  - Briefly describe company, its values, mission, and target audience.\n",
    "  - Highlight any relevant brand guidelines or messaging frameworks.\n",
    "  - Provide a concise overview of the campaign's objectives and goals.\n",
    "  - Briefly explain the product or service being promoted.\n",
    "  - Define your ideal customer with clear demographics, psychographics, and behavioral insights.\n",
    "  - Understand their needs, wants, motivations, and pain points.\n",
    "  - Clearly articulate the desired outcomes for the campaign.\n",
    "  - Use SMART goals (Specific, Measurable, Achievable, Relevant, and Time-bound) for clarity.\n",
    "  - Define key performance indicators (KPIs) to track progress and success.\n",
    "  - Specify the primary and secondary goals of the campaign.\n",
    "  - Examples include brand awareness, lead generation, sales growth, or website traffic.\n",
    "  - Clearly define what differentiates your product or service from competitors.\n",
    "  - Emphasize the value proposition and unique benefits offered to the target audience.\n",
    "  - Define the desired tone and personality of the campaign messaging.\n",
    "  - Identify the specific channels you will use to reach your target audience.\n",
    "  - Clearly state the desired action you want the audience to take.\n",
    "  - Make it specific, compelling, and easy to understand.\n",
    "  - Identify and analyze your key competitors in the market.\n",
    "  - Understand their strengths and weaknesses, target audience, and marketing strategies.\n",
    "  - Develop a differentiation strategy to stand out from the competition.\n",
    "  - Define how you will track the success of the campaign.\n",
    "  - Utilize relevant KPIs to measure performance and return on investment (ROI).\n",
    "  Give proper bullet points and headlines for the marketing campaign. Do not produce any empty lines.\n",
    "  Be very succinct and to the point.\n",
    "  \"\"\"\n",
    "\n",
    "    config = GenerateContentConfig(temperature=0.8, max_output_tokens=8192)\n",
    "\n",
    "    generate_t2t = st.button(\"Generate my campaign\", key=\"generate_campaign\")\n",
    "    if generate_t2t and prompt:\n",
    "        second_tab1, second_tab2 = st.tabs([\"Campaign\", \"Prompt\"])\n",
    "        with st.spinner(\n",
    "            f\"Generating your marketing campaign using {get_model_name(selected_model)} ...\"\n",
    "        ):\n",
    "            with second_tab1:\n",
    "                response = client.models.generate_content(\n",
    "                    model=selected_model,\n",
    "                    contents=prompt,\n",
    "                    config=config,\n",
    "                ).text\n",
    "                if response:\n",
    "                    st.write(\"Your marketing campaign:\")\n",
    "                    st.write(response)\n",
    "            with second_tab2:\n",
    "                st.code(prompt, language=\"markdown\")\n",
    "\n",
    "with tab3:\n",
    "    st.subheader(\"Image Playground\")\n",
    "\n",
    "    selected_model = st.radio(\n",
    "        \"Select Model:\",\n",
    "        MODELS,\n",
    "        format_func=get_model_name,\n",
    "        key=\"selected_model_image\",\n",
    "        horizontal=True,\n",
    "    )\n",
    "\n",
    "    furniture, oven, er_diagrams, glasses, math_reasoning = st.tabs(\n",
    "        [\n",
    "            \"Furniture recommendation\",\n",
    "            \"Oven instructions\",\n",
    "            \"ER diagrams\",\n",
    "            \"Glasses recommendation\",\n",
    "            \"Math reasoning\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    with furniture:\n",
    "        st.markdown(\n",
    "            \"\"\"In this demo, you will be presented with a scene (e.g., a living room) and will use the Gemini model to perform visual understanding. You will see how Gemini can be used to recommend an item (e.g., a chair) from a list of furniture options as input. You can use Gemini to recommend a chair that would complement the given scene and will be provided with its rationale for such selections from the provided list.\"\"\"\n",
    "        )\n",
    "\n",
    "        room_image_uri = \"https://storage.googleapis.com/github-repo/img/gemini/retail-recommendations/rooms/living_room.jpeg\"\n",
    "        chair_1_image_uri = \"https://storage.googleapis.com/github-repo/img/gemini/retail-recommendations/furnitures/chair1.jpeg\"\n",
    "        chair_2_image_uri = \"https://storage.googleapis.com/github-repo/img/gemini/retail-recommendations/furnitures/chair2.jpeg\"\n",
    "        chair_3_image_uri = \"https://storage.googleapis.com/github-repo/img/gemini/retail-recommendations/furnitures/chair3.jpeg\"\n",
    "        chair_4_image_uri = \"https://storage.googleapis.com/github-repo/img/gemini/retail-recommendations/furnitures/chair4.jpeg\"\n",
    "\n",
    "        st.image(room_image_uri, width=350, caption=\"Image of a living room\")\n",
    "        st.image(\n",
    "            [\n",
    "                chair_1_image_uri,\n",
    "                chair_2_image_uri,\n",
    "                chair_3_image_uri,\n",
    "                chair_4_image_uri,\n",
    "            ],\n",
    "            width=200,\n",
    "            caption=[\"Chair 1\", \"Chair 2\", \"Chair 3\", \"Chair 4\"],\n",
    "        )\n",
    "\n",
    "        st.write(\n",
    "            \"Our expectation: Recommend a chair that would complement the given image of a living room.\"\n",
    "        )\n",
    "        content = [\n",
    "            \"Consider the following chairs:\",\n",
    "            \"chair 1:\",\n",
    "            Part.from_uri(file_uri=chair_1_image_uri, mime_type=\"image/jpeg\"),\n",
    "            \"chair 2:\",\n",
    "            Part.from_uri(file_uri=chair_2_image_uri, mime_type=\"image/jpeg\"),\n",
    "            \"chair 3:\",\n",
    "            Part.from_uri(file_uri=chair_3_image_uri, mime_type=\"image/jpeg\"),\n",
    "            \"and\",\n",
    "            \"chair 4:\",\n",
    "            Part.from_uri(file_uri=chair_4_image_uri, mime_type=\"image/jpeg\"),\n",
    "            \"\\n\"\n",
    "            \"For each chair, explain why it would be suitable or not suitable for the following room:\",\n",
    "            Part.from_uri(file_uri=room_image_uri, mime_type=\"image/jpeg\"),\n",
    "            \"Only recommend for the room provided and not other rooms. Provide your recommendation in a table format with chair name and reason as columns.\",\n",
    "        ]\n",
    "\n",
    "        tab1, tab2 = st.tabs([\"Response\", \"Prompt\"])\n",
    "        generate_image_description = st.button(\n",
    "            \"Generate recommendation....\", key=\"generate_image_description\"\n",
    "        )\n",
    "        with tab1:\n",
    "            if generate_image_description and content:\n",
    "                with st.spinner(\n",
    "                    f\"Generating recommendation using {get_model_name(selected_model)} ...\"\n",
    "                ):\n",
    "                    response = client.models.generate_content(\n",
    "                        model=selected_model,\n",
    "                        contents=content,\n",
    "                        config=config,\n",
    "                    ).text\n",
    "                    st.markdown(response)\n",
    "        with tab2:\n",
    "            st.write(\"Prompt used:\")\n",
    "            st.code(content, language=\"markdown\")\n",
    "\n",
    "    with oven:\n",
    "        stove_screen_uri = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/stove.jpg\"\n",
    "        st.write(\n",
    "            \"Equipped with the ability to extract information from visual elements on screens, Gemini can analyze screenshots, icons, and layouts to provide a holistic understanding of the depicted scene.\"\n",
    "        )\n",
    "        st.image(stove_screen_uri, width=350, caption=\"Image of a oven\")\n",
    "        st.write(\n",
    "            \"Our expectation: Provide instructions for resetting the clock on this appliance in English\"\n",
    "        )\n",
    "        prompt = \"\"\"How can I reset the clock on this appliance? Provide the instructions in English.\n",
    "If instructions include buttons, also explain where those buttons are physically located.\n",
    "\"\"\"\n",
    "        tab1, tab2 = st.tabs([\"Response\", \"Prompt\"])\n",
    "        generate_instructions_description = st.button(\n",
    "            \"Generate instructions\", key=\"generate_instructions_description\"\n",
    "        )\n",
    "        with tab1:\n",
    "            if generate_instructions_description and prompt:\n",
    "                with st.spinner(\n",
    "                    f\"Generating instructions using {get_model_name(selected_model)}...\"\n",
    "                ):\n",
    "                    response = client.models.generate_content(\n",
    "                        model=selected_model,\n",
    "                        contents=[\n",
    "                            Part.from_uri(\n",
    "                                file_uri=stove_screen_uri, mime_type=\"image/jpeg\"\n",
    "                            ),\n",
    "                            prompt,\n",
    "                        ],\n",
    "                    ).text\n",
    "                    st.markdown(response)\n",
    "        with tab2:\n",
    "            st.write(\"Prompt used:\")\n",
    "            st.code(prompt, language=\"markdown\")\n",
    "\n",
    "    with er_diagrams:\n",
    "        er_diag_uri = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/er.png\"\n",
    "        st.write(\n",
    "            \"Gemini multimodal capabilities empower it to comprehend diagrams and take actionable steps, such as optimization or code generation. The following example demonstrates how Gemini can decipher an Entity Relationship (ER) diagram.\"\n",
    "        )\n",
    "        st.image(er_diag_uri, width=350, caption=\"Image of an ER diagram\")\n",
    "        st.write(\n",
    "            \"Our expectation: Document the entities and relationships in this ER diagram.\"\n",
    "        )\n",
    "        prompt = \"\"\"Document the entities and relationships in this ER diagram.\n",
    "        \"\"\"\n",
    "        tab1, tab2 = st.tabs([\"Response\", \"Prompt\"])\n",
    "        er_diag_img_description = st.button(\"Generate!\", key=\"er_diag_img_description\")\n",
    "        with tab1:\n",
    "            if er_diag_img_description and prompt:\n",
    "                with st.spinner(\"Generating...\"):\n",
    "                    response = client.models.generate_content(\n",
    "                        model=selected_model,\n",
    "                        contents=[\n",
    "                            Part.from_uri(file_uri=er_diag_uri, mime_type=\"image/jpeg\"),\n",
    "                            prompt,\n",
    "                        ],\n",
    "                    ).text\n",
    "        with tab2:\n",
    "            st.write(\"Prompt used:\")\n",
    "            st.code(prompt, language=\"markdown\")\n",
    "\n",
    "    with glasses:\n",
    "        compare_img_1_uri = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses1.jpg\"\n",
    "        compare_img_2_uri = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses2.jpg\"\n",
    "\n",
    "        st.write(\n",
    "            \"\"\"Gemini is capable of image comparison and providing recommendations. This can be useful in industries like e-commerce and retail.\n",
    "          Below is an example of choosing which pair of glasses would be better suited to various face types:\"\"\"\n",
    "        )\n",
    "        face_type = st.radio(\n",
    "            \"What is your face shape?\",\n",
    "            [\"Oval\", \"Round\", \"Square\", \"Heart\", \"Diamond\"],\n",
    "            key=\"face_type\",\n",
    "            horizontal=True,\n",
    "        )\n",
    "        output_type = st.radio(\n",
    "            \"Select the output type\",\n",
    "            [\"text\", \"table\", \"json\"],\n",
    "            key=\"output_type\",\n",
    "            horizontal=True,\n",
    "        )\n",
    "        st.image(\n",
    "            [compare_img_1_uri, compare_img_2_uri],\n",
    "            width=350,\n",
    "            caption=[\"Glasses type 1\", \"Glasses type 2\"],\n",
    "        )\n",
    "        st.write(\n",
    "            f\"Our expectation: Suggest which glasses type is better for the {face_type} face shape\"\n",
    "        )\n",
    "        content = [\n",
    "            f\"\"\"Which of these glasses you recommend for me based on the shape of my face:{face_type}?\n",
    "      I have an {face_type} shape face.\n",
    "      Glasses 1: \"\"\",\n",
    "            Part.from_uri(file_uri=compare_img_1_uri, mime_type=\"image/jpeg\"),\n",
    "            \"\"\"\n",
    "      Glasses 2: \"\"\",\n",
    "            Part.from_uri(file_uri=compare_img_2_uri, mime_type=\"image/jpeg\"),\n",
    "            f\"\"\"\n",
    "      Explain how you made to this decision.\n",
    "      Provide your recommendation based on my face shape, and reasoning for each in {output_type} format.\n",
    "      \"\"\",\n",
    "        ]\n",
    "        tab1, tab2 = st.tabs([\"Response\", \"Prompt\"])\n",
    "        compare_img_description = st.button(\n",
    "            \"Generate recommendation!\", key=\"compare_img_description\"\n",
    "        )\n",
    "        with tab1:\n",
    "            if compare_img_description and content:\n",
    "                with st.spinner(\n",
    "                    f\"Generating recommendations using {get_model_name(selected_model)}...\"\n",
    "                ):\n",
    "                    response = client.models.generate_content(\n",
    "                        model=selected_model,\n",
    "                        contents=[\n",
    "                            Part.from_uri(file_uri=er_diag_uri, mime_type=\"image/jpeg\"),\n",
    "                            content,\n",
    "                        ],\n",
    "                    ).text\n",
    "                    st.markdown(response)\n",
    "        with tab2:\n",
    "            st.write(\"Prompt used:\")\n",
    "            st.code(content, language=\"markdown\")\n",
    "\n",
    "    with math_reasoning:\n",
    "        math_image_uri = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/math_beauty.jpg\"\n",
    "\n",
    "        st.write(\n",
    "            \"Gemini can also recognize math formulas and equations and extract specific information from them. This capability is particularly useful for generating explanations for math problems, as shown below.\"\n",
    "        )\n",
    "        st.image(math_image_uri, width=350, caption=\"Image of a math equation\")\n",
    "        st.markdown(\n",
    "            \"\"\"\n",
    "        Our expectation: Ask questions about the math equation as follows:\n",
    "        - Extract the formula.\n",
    "        - What is the symbol right before Pi? What does it mean?\n",
    "        - Is this a famous formula? Does it have a name?\n",
    "          \"\"\"\n",
    "        )\n",
    "        prompt = \"\"\"\n",
    "Follow the instructions.\n",
    "Surround math expressions with $.\n",
    "Use a table with a row for each instruction and its result.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Extract the formula.\n",
    "- What is the symbol right before Pi? What does it mean?\n",
    "- Is this a famous formula? Does it have a name?\n",
    "\"\"\"\n",
    "        tab1, tab2 = st.tabs([\"Response\", \"Prompt\"])\n",
    "        math_image_description = st.button(\n",
    "            \"Generate answers!\", key=\"math_image_description\"\n",
    "        )\n",
    "        with tab1:\n",
    "            if math_image_description and prompt:\n",
    "                with st.spinner(\n",
    "                    f\"Generating answers for formula using {get_model_name(selected_model)}...\"\n",
    "                ):\n",
    "                    response = client.models.generate_content(\n",
    "                        model=selected_model,\n",
    "                        contents=[\n",
    "                            Part.from_uri(\n",
    "                                file_uri=math_image_uri, mime_type=\"image/jpeg\"\n",
    "                            ),\n",
    "                            prompt,\n",
    "                        ],\n",
    "                    ).text\n",
    "                    st.markdown(response)\n",
    "                    st.markdown(\"\\n\\n\\n\")\n",
    "        with tab2:\n",
    "            st.write(\"Prompt used:\")\n",
    "            st.code(prompt, language=\"markdown\")\n",
    "\n",
    "with tab4:\n",
    "    st.subheader(\"Video Playground\")\n",
    "\n",
    "    selected_model = st.radio(\n",
    "        \"Select Model:\",\n",
    "        MODELS,\n",
    "        format_func=get_model_name,\n",
    "        key=\"selected_model_video\",\n",
    "        horizontal=True,\n",
    "    )\n",
    "\n",
    "    vide_desc, video_tags, video_highlights, video_geolocation = st.tabs(\n",
    "        [\"Video description\", \"Video tags\", \"Video highlights\", \"Video geolocation\"]\n",
    "    )\n",
    "\n",
    "    with vide_desc:\n",
    "        st.markdown(\n",
    "            \"\"\"Gemini can also provide the description of what is going on in the video:\"\"\"\n",
    "        )\n",
    "        video_desc_uri = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/mediterraneansea.mp4\"\n",
    "\n",
    "        if video_desc_uri:\n",
    "            st.video(video_desc_uri)\n",
    "            st.write(\"Our expectation: Generate the description of the video\")\n",
    "            prompt = \"\"\"Describe what is happening in the video and answer the following questions: \\n\n",
    "      - What am I looking at? \\n\n",
    "      - Where should I go to see it? \\n\n",
    "      - What are other top 5 places in the world that look like this?\n",
    "      \"\"\"\n",
    "            tab1, tab2 = st.tabs([\"Response\", \"Prompt\"])\n",
    "            vide_desc_description = st.button(\n",
    "                \"Generate video description\", key=\"vide_desc_description\"\n",
    "            )\n",
    "            with tab1:\n",
    "                if vide_desc_description and prompt:\n",
    "                    with st.spinner(\n",
    "                        f\"Generating video description using {get_model_name(selected_model)} ...\"\n",
    "                    ):\n",
    "                        response = client.models.generate_content(\n",
    "                            model=selected_model,\n",
    "                            contents=[\n",
    "                                Part.from_uri(\n",
    "                                    file_uri=video_desc_uri, mime_type=\"video/mp4\"\n",
    "                                ),\n",
    "                                prompt,\n",
    "                            ],\n",
    "                        ).text\n",
    "                        st.markdown(response)\n",
    "                        st.markdown(\"\\n\\n\\n\")\n",
    "            with tab2:\n",
    "                st.write(\"Prompt used:\")\n",
    "                st.code(prompt, language=\"markdown\")\n",
    "\n",
    "    with video_tags:\n",
    "        st.markdown(\n",
    "            \"\"\"Gemini can also extract tags throughout a video, as shown below:.\"\"\"\n",
    "        )\n",
    "        video_tags_uri = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/photography.mp4\"\n",
    "\n",
    "        if video_tags_uri:\n",
    "            st.video(video_tags_uri)\n",
    "            st.write(\"Our expectation: Generate the tags for the video\")\n",
    "            prompt = \"\"\"Answer the following questions using the video only:\n",
    "            1. What is in the video?\n",
    "            2. What objects are in the video?\n",
    "            3. What is the action in the video?\n",
    "            4. Provide 5 best tags for this video?\n",
    "            Give the answer in the table format with question and answer as columns.\n",
    "      \"\"\"\n",
    "            tab1, tab2 = st.tabs([\"Response\", \"Prompt\"])\n",
    "            video_tags_description = st.button(\n",
    "                \"Generate video tags\", key=\"video_tags_description\"\n",
    "            )\n",
    "            with tab1:\n",
    "                if video_tags_description and prompt:\n",
    "                    with st.spinner(\n",
    "                        f\"Generating video description using {get_model_name(selected_model)} ...\"\n",
    "                    ):\n",
    "                        response = client.models.generate_content(\n",
    "                            model=selected_model,\n",
    "                            contents=[\n",
    "                                Part.from_uri(\n",
    "                                    file_uri=video_tags_uri, mime_type=\"video/mp4\"\n",
    "                                ),\n",
    "                                prompt,\n",
    "                            ],\n",
    "                        ).text\n",
    "                        st.markdown(response)\n",
    "                        st.markdown(\"\\n\\n\\n\")\n",
    "            with tab2:\n",
    "                st.write(\"Prompt used:\")\n",
    "                st.code(prompt, language=\"markdown\")\n",
    "\n",
    "    with video_highlights:\n",
    "        st.markdown(\n",
    "            \"\"\"Below is another example of using Gemini to ask questions about objects, people or the context, as shown in the video about Pixel 8 below:\"\"\"\n",
    "        )\n",
    "        video_highlights_uri = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4\"\n",
    "\n",
    "        if video_highlights_uri:\n",
    "            st.video(video_highlights_uri)\n",
    "            st.write(\"Our expectation: Generate the highlights for the video\")\n",
    "            prompt = \"\"\"Answer the following questions using the video only:\n",
    "What is the profession of the girl in this video?\n",
    "Which all features of the phone are highlighted here?\n",
    "Summarize the video in one paragraph.\n",
    "Provide the answer in table format.\n",
    "      \"\"\"\n",
    "            tab1, tab2 = st.tabs([\"Response\", \"Prompt\"])\n",
    "            video_highlights_description = st.button(\n",
    "                \"Generate video highlights\", key=\"video_highlights_description\"\n",
    "            )\n",
    "            with tab1:\n",
    "                if video_highlights_description and prompt:\n",
    "                    with st.spinner(\n",
    "                        f\"Generating video highlights using {get_model_name(selected_model)} ...\"\n",
    "                    ):\n",
    "                        response = client.models.generate_content(\n",
    "                            model=selected_model,\n",
    "                            contents=[\n",
    "                                Part.from_uri(\n",
    "                                    file_uri=video_highlights_uri, mime_type=\"video/mp4\"\n",
    "                                ),\n",
    "                                prompt,\n",
    "                            ],\n",
    "                        ).text\n",
    "                        st.markdown(response)\n",
    "                        st.markdown(\"\\n\\n\\n\")\n",
    "            with tab2:\n",
    "                st.write(\"Prompt used:\")\n",
    "                st.code(prompt, language=\"markdown\")\n",
    "\n",
    "    with video_geolocation:\n",
    "        st.markdown(\n",
    "            \"\"\"Even in short, detail-packed videos, Gemini can identify the locations.\"\"\"\n",
    "        )\n",
    "        video_geolocation_uri = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/bus.mp4\"\n",
    "\n",
    "        if video_geolocation_uri:\n",
    "            st.video(video_geolocation_uri)\n",
    "            st.markdown(\n",
    "                \"\"\"Our expectation: \\n\n",
    "      Answer the following questions from the video:\n",
    "        - What is this video about?\n",
    "        - How do you know which city it is?\n",
    "        - What street is this?\n",
    "        - What is the nearest intersection?\n",
    "      \"\"\"\n",
    "            )\n",
    "            prompt = \"\"\"Answer the following questions using the video only:\n",
    "      What is this video about?\n",
    "      How do you know which city it is?\n",
    "      What street is this?\n",
    "      What is the nearest intersection?\n",
    "      Answer the following questions in a table format with question and answer as columns.\n",
    "      \"\"\"\n",
    "            tab1, tab2 = st.tabs([\"Response\", \"Prompt\"])\n",
    "            video_geolocation_description = st.button(\n",
    "                \"Generate\", key=\"video_geolocation_description\"\n",
    "            )\n",
    "            with tab1:\n",
    "                if video_geolocation_description and prompt:\n",
    "                    with st.spinner(\n",
    "                        f\"Generating location tags using {get_model_name(selected_model)} ...\"\n",
    "                    ):\n",
    "                        response = client.models.generate_content(\n",
    "                            model=selected_model,\n",
    "                            contents=[\n",
    "                                Part.from_uri(\n",
    "                                    file_uri=video_geolocation_uri,\n",
    "                                    mime_type=\"video/mp4\",\n",
    "                                ),\n",
    "                                prompt,\n",
    "                            ],\n",
    "                        ).text\n",
    "                        st.markdown(response)\n",
    "                        st.markdown(\"\\n\\n\\n\")\n",
    "            with tab2:\n",
    "                st.write(\"Prompt used:\")\n",
    "                st.code(prompt, language=\"markdown\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
